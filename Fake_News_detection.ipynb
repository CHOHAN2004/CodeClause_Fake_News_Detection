{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "_6D5bXe_zi1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f496b14-8a81-4ce3-aea0-76bae1a3a866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import arange\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "import re  # Data Preprocessing\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "!pip install --upgrade gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import gensim.downloader\n",
        "\n",
        "pre_ft_vectors = gensim.downloader.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Import the appropriate vectorizers (CountVect., and TF-IDF Vect.)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer as cvect\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer as tfvect\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier as rfc\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold as kfold\n",
        "\n",
        "# Import spacy for the lemmatisation process\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "from wordcloud import WordCloud  # Data Visualization\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Ignore warnings\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_df=pd.read_csv('/content/sample_data/Fake.csv')\n",
        "fake_df=pd.read_csv('/content/sample_data/True.csv')\n"
      ],
      "metadata": {
        "id": "jl0Ka6ZZ1cBp"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0.2  # Keep 20% of the data.\n",
        "true_df = true_df.head(int(p * true_df.shape[0]))\n",
        "fake_df = fake_df.head(int(p * fake_df.shape[0]))"
      ],
      "metadata": {
        "id": "Y331rLTH1ptg"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_string(sent):\n",
        "    sent = sent.lower()\n",
        "    sent = re.sub('\\n|\\r|\\t', '', sent)   # Remove whitespace chars\n",
        "    sent = re.sub(r'[^\\w\\s]+', '', sent)  # Remove punctuation\n",
        "    return sent\n",
        "\n",
        "def preprocess(df):\n",
        "    df.dropna(subset = ['title', 'text'], inplace = True)  # Remove rows with missing values in either title or text\n",
        "    vfunc = np.vectorize(clean_string)    # Speed up string clean-up using vectorization\n",
        "    df['title'] = vfunc(df['title'])\n",
        "    df['text'] = vfunc(df['text'])\n",
        "    return df\n",
        ""
      ],
      "metadata": {
        "id": "eHoYipPO1r0L"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "true_df = preprocess(true_df)\n",
        "fake_df = preprocess(fake_df)"
      ],
      "metadata": {
        "id": "wihe5SUi2vI_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(string):\n",
        "  lem = [token.lemma_ for token in nlp(string, disable=[\"parser\", \"ner\"]) if token.lemma_ != '-PRON-']\n",
        "  string = \" \".join(lem)\n",
        "  return string\n",
        "\n",
        "# Lemmatise all of the titles and descriptions\n",
        "def lemmatize_title_text(df):\n",
        "    df_lem = df.copy()\n",
        "    vfunc = np.vectorize(lemmatize)    # Speed up string clean-up using vectorization\n",
        "    df_lem['text']  = vfunc(df_lem['text'])\n",
        "    df_lem['title'] = vfunc(df_lem['title'])\n",
        "    return df_lem\n",
        "\n",
        "# Extract descriptions and titles and merge them into individual strings\n",
        "def extract_title_text(df):\n",
        "    texts = \" \".join(df['text'].to_list()).strip()\n",
        "    titles = \" \".join(df['title'].to_list()).strip()\n",
        "    return (titles, texts)\n",
        "\n",
        "def lemmatize_and_extract(df):\n",
        "    lem_df = lemmatize_title_text(df)\n",
        "    titles_texts = extract_title_text(lem_df)\n",
        "    return lem_df, titles_texts"
      ],
      "metadata": {
        "id": "irgEo-uSAPcm"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_df_lem, true_tt = lemmatize_and_extract(true_df)\n",
        "fake_df_lem, fake_tt = lemmatize_and_extract(fake_df)"
      ],
      "metadata": {
        "id": "0rxJBYuZASKV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_title_text(df):\n",
        "    content = zip( df['title'].to_list(), df['text'].to_list() )\n",
        "    df['content'] = [ str(title + text) for title, text in content ]\n",
        "\n",
        "# Create \"content\" field, add \"label\" field and drop every other pre-existing field.\n",
        "def prepare_df(df, label):\n",
        "    df['label'] = [ label for i in range(df.shape[0]) ]  # Add label\n",
        "    merge_title_text(df)\n",
        "    df.drop(columns = ['title', 'text', 'subject', 'date'], inplace=True)"
      ],
      "metadata": {
        "id": "gGn9RWa0AY22"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_df(true_df_lem, 1)\n",
        "prepare_df(fake_df_lem, 0)\n",
        ""
      ],
      "metadata": {
        "id": "ATZ1kjybAd1R"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined = pd.concat([true_df_lem, fake_df_lem])\n",
        "train_df, test_df = train_test_split(combined, train_size=0.5, random_state=420)\n",
        "\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Store dataframes in .csv format.\n",
        "test_df.to_csv (path_or_buf='test.csv',  columns=test_df.columns)\n",
        "train_df.to_csv(path_or_buf='train.csv', columns=train_df.columns)"
      ],
      "metadata": {
        "id": "oH2MUHKRAs04"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_representation(vectorizer, train_df, test_df):\n",
        "    # Collect labels\n",
        "    y_train = train_df['label']\n",
        "    y_test = test_df['label']\n",
        "    # Vectorize training and testing set.\n",
        "    # We use fit_transform() on the training set, in order to learn\n",
        "    # the parameters of scaling on the training set and in the same\n",
        "    # time we scale the train data. We only use transform() on\n",
        "    # the testing set, because we use the scaling paramaters learned\n",
        "    # on the train data to scale the test data.\n",
        "    x_train = vectorizer.fit_transform(train_df['content'])\n",
        "    x_test = vectorizer.transform(test_df['content'])\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "Wny7HRZ948B9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_rep = get_representation(CountVectorizer(), train_df, test_df)\n",
        "tf_rep = get_representation(TfidfVectorizer(), train_df, test_df)"
      ],
      "metadata": {
        "id": "iq-iz7DA_9f8"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __classify(method, x_train, y_train, x_test, y_test, wmean = False):\n",
        "    # Make a pipeline consisting of a scaler and the classification method\n",
        "    pipe = make_pipeline(StandardScaler(with_mean=wmean), method)\n",
        "    # Apply scaling on training data\n",
        "    pipe.fit(x_train, y_train)\n",
        "    # Apply scaling on testing data, without leaking training data.\n",
        "    pipe.score(x_test, y_test)\n",
        "\n",
        "    y_pred = pipe.predict(x_test)\n",
        "    # Print the report\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    accuracy = accuracy_score(y_test, y_pred);\n",
        "    print('Accuracy is {:.2f} %'.format(accuracy*100))\n",
        "\n",
        "# Applies a classification method to the given data and prints\n",
        "# the classification report and the accuracy of the classifier.\n",
        "# Receives either a CountVectorizer() representation or\n",
        "# a TfidfVectorizer() representation.\n",
        "def classify(method, cv_rep, tf_rep, tfid = False, toarr = False):\n",
        "    # Extract the sets\n",
        "    x_train, y_train, x_test, y_test = tf_rep if tfid else cv_rep\n",
        "    # Convert to array if needed\n",
        "    if toarr: x_train, x_test = x_train.toarray(), x_test.toarray()\n",
        "\n",
        "    __classify(method, x_train, y_train, x_test, y_test, False)"
      ],
      "metadata": {
        "id": "VyK1yqjgAXDQ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sim_logistic_regression(cv_rep, tf_rep, tfid = False):\n",
        "    classify(LogisticRegression(), cv_rep, tf_rep, tfid, False)"
      ],
      "metadata": {
        "id": "CAJbUYc6CO39"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_logistic_regression(cv_rep, tf_rep, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAU43ADUCR87",
        "outputId": "66d587e4-e2c5-42cb-cad0-1a0899e27721"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97      2073\n",
            "           1       0.99      0.96      0.97      2417\n",
            "\n",
            "    accuracy                           0.97      4490\n",
            "   macro avg       0.97      0.97      0.97      4490\n",
            "weighted avg       0.97      0.97      0.97      4490\n",
            "\n",
            "Accuracy is 97.31 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_logistic_regression(cv_rep, tf_rep, True)"
      ],
      "metadata": {
        "id": "zRLsOcDuCYVB",
        "outputId": "9d8f1d08-0b0f-40fc-821f-7c9fab8faee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98      2073\n",
            "           1       1.00      0.97      0.98      2417\n",
            "\n",
            "    accuracy                           0.98      4490\n",
            "   macro avg       0.98      0.98      0.98      4490\n",
            "weighted avg       0.98      0.98      0.98      4490\n",
            "\n",
            "Accuracy is 98.35 %\n"
          ]
        }
      ]
    }
  ]
}